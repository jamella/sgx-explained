\section{Background}
\label{sec:background}

Arguing about the security of an application running on an mainstream computer
using the Intel architecture requires understanding the interactions between
all the parts of an x86 execution environment. This section provides an
overview of the features referenced by the rest of the paper. Unless specified
otherwise, the information in this section can be found in Intel's
\textit{Software Development Manual} \cite{intel2014sdm} (SDM).

Each of the sub-sections below explains how its information is relevant to
to SGX, but does not introduce any SGX concepts. Experienced readers can safely
skip this section and refer back if necessary.


\input{contents/background/rings.tex}
\input{contents/background/paging.tex}
\input{contents/background/registers.tex}
\input{contents/background/computer.tex}
\input{contents/background/microcode.tex}
\input{contents/background/caching.tex}
\input{contents/background/cache_org.tex}



\subsection{Cache Coherence}
\label{sec:cache_coherence}

The Intel architecture was designed to support application software that was
not written with caches in mind. One aspect of this is the Total Store Order
(TSO) \cite{owens2009tso} memory model, the guarantee that concurrently running
hardware threads see the same order of DRAM writes. Given that a memory
location might be copied in multiple caches on different cores, or even
different processors, providing the TSO guarantees requires a \textit{cache
coherence protocol} that keeps the cache line copies in sync. This section
covers some cache coherence implementation details that are necessary for
understanding SGX. \cite{hennessy2012architecture} provides a good introduction
to cache coherence principles.

The cache coherence mechanism is not visible to software, so it only briefly
mentioned in the SDM. Fortunately, Intel's optimization reference
\cite{intel2014optimization} and the datasheets referenced in
\S~\ref{sec:cpu_die} provide more information. Intel processors use variations
of the MESIF \cite{goodman2009mesif} protocol, which is implemented in the CPU
and in the protocol layer of the QPI bus.

The SDM and the CPUID instructions indicate that the L3 cache, also known as
the \textit{last-level cache} (LLC) is \textit{inclusive}, meaning that any
location cached by an L1 or L2 cache must also be cached in the LLC. This
design decision reduces complexity in many implementation aspects. We estimate
that the bulk of the cache coherence implementation is in the CPU's uncore,
because cache synchronization can be achieved without having to communicate to
the lower cache levels, which are inside execution cores.

Unfortunately, a cache timing attack can take advantage of the fact that the
LLC is inclusive and shared among CPU cores. This allows an attacker thread
to monitor a target thread that runs on a core in the same CPU die. The
attacker can evict lines in the target core's cache by filling up the L3 cache,
and then probe the L3 cache to find out when the target causes cache evictions.
The evicted lines disclose some bits in the target thread's memory accesses.

The QPI protocol defines \textit{cache agents}, which are connected to the
last-level cache in a processor, and \textit{home agents}, which are connected
to memory controllers. Cache agents make requests to home agents for cache line
data on cache misses, while home agents keep track of cache line ownership, and
obtain the cache line data from other cache line agents, or from the memory
controller. The QPI routing layer supports multiple agents per socket, and each
processor has its own caching agents, and at least one home agent.

The CPU uncore (see Figure~\ref{fig:cpu_die}) has a bidirectional ring
interconnect used for communication between execution cores and the other
uncore components. The ring has one CBox connected to each core. Each CBox also
connects an LLC slice to the ring, and serves as the QPI cache agent for that
L3 cache slice.


\cite{intel2014datasheet} states that the LLC coherence engine is implemented
in the

According to Intel's patents, the SGX memory protection relies on special
entries in the \textit{Source Address Decoder} (SAD) and \textit{Target Address
Decoder} (TAD).  A thorough understanding of the memory hierarchy is required
in order to understand this aspect of the SGX implementation.

UBox - uncore configuration controller; master for reading and writing
physically distributed registers across the uncore using the message
channel; receives interrupts from system and dispatches them to the
appropriate core; system lock master (e.g. QPI bus lock)

CBox - last-level cache (LLC) coherence engine; the interface between a core
and a slice of the LLC; acts as the QPI cache agent for that slice of LLC;
CBoxes are co-located with cores and connected in a ring interconnect

The physical memory space is split up between CBoxes. The ``complex'' caching
algorithm mentioned in the Intel SDM includes a ``hashing'' step that maps a
physical address to a CBox, and thus a slice of the LLC.

Each CBox contains a Source Address Decoder (SAD), and the configurations of
all SADs in a core are identical, replicated by the UBox. The SAD takes in a
memory address and access type, and outputs a transaction type (coherent,
non-coherent, IO) and a node ID.

Home agent - contains the Target Address Decoder (TAD) and interfaces with a
memory controller; a CPU may contain multiple home agents, if it has multiple
memory contollers; the TAD maps a memory address to a specific DRAM channel,
implements logical channel address mapping and interleaving;

\input{contents/background/tlbs.tex}

