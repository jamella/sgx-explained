\HeadingLevelB{Cache Timing Attacks}
\label{sec:cache_timing}

Cache timing attacks~\cite{banescu2011cache} are a powerful class of software
attacks that can be mounted entirely by application code running at ring 3
(\S~\ref{sec:rings}). Cache timing attacks do not learn information by reading
the victim's memory, so they bypass the address translation-based isolation
measures (\S~\ref{sec:paging}) implemented in today's kernels and hypervisors.


\HeadingLevelC{Theory}

Cache timing attacks exploit the unfortunate dependency between the location of
a memory access and the time it takes to perform the access. A cache miss
requires at least one memory access to the next level cache, and might require
a second memory access if a write-back occurs. On the Intel architecture, the
latency between a cache hit and a miss can be easily measured by the
\texttt{RDTSC} and \texttt{RDTSCP} instructions (\S~\ref{sec:address_spaces}),
which read a high-resolution time-stamp counter. These instructions have been
designed for benchmarking and optimizing software, so they are available to
ring 3 software.

The fundamental tool of a cache timing attack is an attacker process that
measures the latency of accesses to carefully designated memory locations in
its own address space. The memory locations are chosen so that they map to
the same cache lines as some interesting memory locations in a victim process,
in a cache that is shared between the attacker and the victim. This requires
in-depth knowledge of the shared cache's organization (\S~\ref{sec:cache_org}).

Armed with the knowledge of the cache's organization, the attacker process
sets up the attack by accessing its own memory in such a way that it fills up
all the ways in the cache sets that would hold the victim's interesting memory
locations. After the targeted cache sets are full, the attacker allows the
victim process to execute. When the victim process accesses an interesting
memory locations in its own address space, the shared cache must evict one of
the cache lines holding the attacker's memory locations.

As the victim is executing, the attacker process repeatedly times accesses to
its own memory locations. When the access times indicate that a location was
evicted from the cache, the attacker can conclude that the victim accessed an
interesting memory location in its own cache. Over time, the attacker collects
the results of many measurements and learns a subset of the victim's memory
access pattern. If the victim processes sensitive information using
data-dependent memory fetches, the attacker may be able to deduce the sensitive
information from the learned memory access pattern.


\HeadingLevelC{Practical Considerations}

Cache timing attacks require control over a software process that shares a
cache memory with the victim process. Therefore, a cache timing attack that
aims at the L2 cache would have to rely on the system software to schedule a
software thread on a logical processor in the same core as the target software,
whereas an attack on the L3 cache can be performed using any logical processor
on the same CPU. The latter attacks rely on the fact that the L3 cache is
inclusive, which greatly simplifies the processor's cache coherence
implementation (\S~\ref{sec:cache_coherence}).

The cache sharing requirement implies that L3 cache attacks are feasible in an
IaaS environment, whereas L2 cache attacks become a significant concern when
running sensitive software on a user's desktop.

Out-of-order execution (\S~\ref{sec:out_of_order}) can introduce noise in cache
timing attacks. First, memory accesses may not be performed in program order,
which can impact the lines selected by the cache eviction algorithms. Second,
out-of-order execution may result in cache fills that do not correspond to
executed instructions. For example, a load that follows a faulting instruction
may be scheduled and executed before the fault is detected.

Cache timing attacks must account for speculative execution, as mispredicted
memory accesses can still cause cache fills. Therefore, the attacker may
observe cache fills that don't correspond to instructions that were actually
executed by the victim software. Memory prefetching adds further noise to cache
timing attacks, as the attacker may observe cache fills that don't correspond
to instructions in the victim code, even when accounting for speculative
execution.


\HeadingLevelC{Known Cache Timing Attacks}

Despite these difficulties, cache timing attacks are known to retrieve
cryptographic keys used by AES~\cite{osvik2006aes, bonneau2006aes},
RSA~\cite{brumley2005rsa}, Diffie-Hellman~\cite{kocher1996timing}, and
elliptic-curve cryptography~\cite{brumley2011ecc}.

Early attacks required access to the victim's CPU core, but more sophisticated
recent attacks~\cite{yarom2013llctiming, liu2015llctiming} are able to use the
L3 cache, which is shared by all the cores on a CPU die. L3-based attacks can
be particularly devastating in cloud computing scenarios, where running
software on the same computer as a victim application only requires modest
statistical analysis skills and a small amount of
money~\cite{ristenpart2009colocation}. Furthermore, cache timing attacks were
recently demonstrated using JavaScript code in a page visited by a Web
browser~\cite{oren2015jstiming}.

Given this pattern of vulnerabilities, ignoring cache timing attacks is
dangerously similar to ignoring the string of demonstrated attacks which led to
the deprecation of SHA-1~\cite{nist2014sha1policy, google2014sha1deprecation,
microsoft2014sha1deprecation}.


\HeadingLevelC{Defending against Cache Timing Attacks}
\label{sec:cache_timing_workarounds}

Fortunately, invalidating any of the preconditions for cache timing attacks is
sufficient for defending against them. The easiest precondition to focus on is
that the attacker must have access to memory locations that map to the same
sets in a cache as the victim's memory. This assumption can be invalidated by
the judicious use of a cache partitioning scheme.

Performance concerns aside, the main difficulty associated with cache
partitioning schemes is that they must be implemented by a trusted party. When
the system software is trusted, it can (for example) use the principles behind
page coloring~\cite{taylor1990coloring, kessler1992coloring} to partition the
caches~\cite{lin2008coloring} between mutually distrusting parties. This comes
down setting up the page tables in such a way that no two mutually distrusting
software module are stored in physical pages that map to the same sets in any
cache memory.  However, if the system software is not trusted, the cache
partitioning scheme must be implemented in hardware.

The other interesting precondition is that the victim must access its memory in
a data-dependent fashion that allows the attacker to infer private information
from the observed memory access pattern. It becomes tempting to think that
cache timing attacks can be prevented by eliminating data-dependent memory
accesses from all the code handling sensitive data.

However, removing data-dependent memory accesses is difficult to accomplish in
practice because instruction fetches must also be taken into consideration.
\cite{kasper2009aes} gives an idea of the level of effort required for removing
data-dependent accesses from AES, which is a relatively simple data processing
algorithm. At the time of this writing, we are not aware of any approach that
scales to large pieces of software.

While the focus of this section is cache timing attacks, we would like to point
out that any shared resource can lead to information leakage. A worrying
example is hyper-threading (\S~\ref{sec:cpu_core}), where each CPU core is
represented as two logical processors, and the threads executing on these two
processors share execution units. An attacker that can run a process on a
logical processor sharing a core with a victim process can use
\texttt{RDTSCP}~\cite{petters1999making} to learn which execution units are in
use, and infer what instructions are executed by the victim process.
